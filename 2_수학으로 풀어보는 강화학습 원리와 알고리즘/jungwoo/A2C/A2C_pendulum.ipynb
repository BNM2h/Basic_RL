{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_space):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_space, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.mu_head = nn.Linear(16, 1)\n",
    "        self.std_head = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        mu = torch.tanh(self.mu_head(x))*2 ## tanh -1.0 ~ 1.0 이라서. 대신 mu값이 -2.0 < mu < 2.0 이여야 한다.\n",
    "        std = F.softplus(self.std_head(x)) ## softplus 함수 보면 relu 부드러운 버전임. std가 0이 되지 않도록하기 위해 softplus 씀.\n",
    "        return mu, std\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_space):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_space, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        v = self.fc4(x)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_pdf(mu, std, action): ## origin style\n",
    "    std = std.clamp(0.01, 1.0) # 표준편차의 최솟값 최댓값. 1로 고정해도 되지만.. 아무래도 좀 더 유연성을 주는게 좋아보임.\n",
    "    var = std**2 # 분산. variance\n",
    "    log_policy_pdf = -0.5*(action-mu)**2/var - 0.5*torch.log(var*2*torch.asin(torch.tensor(1.)))# torch.asin(torch.tensor(1.)) = pi. 찾아보니 pytorch는 pi 라이브러리가 없음 ㅠ\n",
    "    return torch.sum(log_policy_pdf, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_pdf_pytorch_ver(mu, std, action): ## torch style\n",
    "    std = std.clamp(0.01, 1.0) # 표준편차의 최솟값 최댓값. 1로 고정해도 되지만.. 아무래도 좀 더 유연성을 주는게 좋아보임.\n",
    "    normal = Normal(mu, std) ## 대표값과 표준편차 값을 통해 normal distribution = gaussian distribution으로 만들어 준다.\n",
    "    log_policy_pdf = normal.log_prob(action) # pytorch에서 log_prob 함수를 통해 action에 맞게 계산해준다.\n",
    "    return log_policy_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(mu, std):\n",
    "    std = std.clamp(0.01, 1.0) # 표준편차의 최솟값 최댓값. 1로 고정해도 되지만.. 아무래도 좀 더 유연성을 주는게 좋아보임.\n",
    "    normal = Normal(mu, std) ## 대표값과 표준편차 값을 통해 normal distribution = gaussian distribution으로 만들어 준다.\n",
    "    action = normal.sample() ## 추정된 mu, std를 통해 만들어진 가우시안 분포도에서 샘플 1개 선택!, 확률적으로 선택됨.\n",
    "    action = action.clamp(-2.0, 2.0) ## 환경에 action이 들어갈땐 허용가능한 범위로 바꿔준다.\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_target(critic, states, rewards, masks):\n",
    "    gamma = 0.99\n",
    "    \n",
    "    target_y = torch.zeros_like(rewards)\n",
    "    values = critic(states).view(-1)\n",
    "    next_value = 0\n",
    "    \n",
    "    for t in reversed(range(0,len(rewards))):\n",
    "        target_y[t] = rewards[t] + gamma*next_value*masks[t] ## 특정 t 시점부터 next~t에 해당하는 reward가 discounted 됨으로 앞으로 가면서 누적 gamma 곱 해준다.\n",
    "        next_value = values.data[t]\n",
    "    return target_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(actor, critic, actor_optimizer, critic_optimizer, replay_buffer):\n",
    "    \n",
    "    \n",
    "    # data 분배\n",
    "    replay_buffer = np.array(replay_buffer) \n",
    "    states = np.vstack(replay_buffer[:, 0]) \n",
    "    actions = list(replay_buffer[:, 1])\n",
    "    rewards = list(replay_buffer[:, 2])\n",
    "    next_states = np.vstack(replay_buffer[:, 3])\n",
    "    masks = list(replay_buffer[:, 4]) \n",
    "\n",
    "    # tensor.\n",
    "    states = torch.Tensor(states)\n",
    "    actions = torch.Tensor(actions).unsqueeze(1)\n",
    "    rewards = torch.Tensor(rewards) \n",
    "    next_states = torch.Tensor(next_states)\n",
    "    masks = torch.Tensor(masks)\n",
    "    \n",
    "    # actor_loss\n",
    "    mu, std = actor(states)\n",
    "    #log_prob = log_pdf(mu, std, action).view(-1) ## G와 shape 맞춰주기 위해서 [data,1] -> [data]\n",
    "    log_prob = log_pdf_pytorch_ver(mu, std, actions).view(-1) ## 위 방식과 같다. 다만 pytorch 라이브러리에 더 의존적.\n",
    "    target_y = td_target(critic, states, rewards, masks)\n",
    "    values = critic(states).view(-1)\n",
    "    \n",
    "    advantages = target_y - values\n",
    "    actor_loss = torch.sum(-log_prob*advantages.detach())\n",
    "    \n",
    "    # critic_loss\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    critic_loss = mse_loss(target_y.detach(), values)\n",
    "    \n",
    "    # backward\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "    \n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  20  score:  -1414.9114498134768\n",
      "episode:  40  score:  -1268.4410006454098\n",
      "episode:  60  score:  -1236.5634665889506\n",
      "episode:  80  score:  -1273.5225299809592\n",
      "episode:  100  score:  -1256.0191640440298\n",
      "episode:  120  score:  -1236.814480385676\n",
      "episode:  140  score:  -1265.0243436459402\n",
      "episode:  160  score:  -1241.9399420233751\n",
      "episode:  180  score:  -1227.6943113938005\n",
      "episode:  200  score:  -1269.0137055581654\n",
      "episode:  220  score:  -1213.6920068734462\n",
      "episode:  240  score:  -1195.2041180740205\n",
      "episode:  260  score:  -1175.238928880251\n",
      "episode:  280  score:  -1203.7430331281507\n",
      "episode:  300  score:  -1261.3430795308261\n",
      "episode:  320  score:  -1246.4221620508508\n",
      "episode:  340  score:  -1243.3974725664043\n",
      "episode:  360  score:  -1254.3187033049019\n",
      "episode:  380  score:  -1214.7394183657777\n",
      "episode:  400  score:  -1184.1807715009531\n",
      "episode:  420  score:  -1207.684997886287\n",
      "episode:  440  score:  -1255.6060666622975\n",
      "episode:  460  score:  -1157.316347155787\n",
      "episode:  480  score:  -1134.69234083129\n",
      "episode:  500  score:  -1188.3665382901877\n",
      "episode:  520  score:  -1136.411032014258\n",
      "episode:  540  score:  -1116.0034031060686\n",
      "episode:  560  score:  -1193.635650046653\n",
      "episode:  580  score:  -1128.065541295101\n",
      "episode:  600  score:  -1126.932147073629\n",
      "episode:  620  score:  -1106.5220223880165\n",
      "episode:  640  score:  -1195.5748119085913\n",
      "episode:  660  score:  -1223.1414828081233\n",
      "episode:  680  score:  -1093.930891144313\n",
      "episode:  700  score:  -1114.8549028476252\n",
      "episode:  720  score:  -1108.1008889254879\n",
      "episode:  740  score:  -1118.7418773477514\n",
      "episode:  760  score:  -1079.2364973002848\n",
      "episode:  780  score:  -1108.0486669704062\n",
      "episode:  800  score:  -1081.7912975718755\n",
      "episode:  820  score:  -1112.3749381107975\n",
      "episode:  840  score:  -1153.4601919256613\n",
      "episode:  860  score:  -1154.350178955878\n",
      "episode:  880  score:  -1200.9233010270823\n",
      "episode:  900  score:  -1136.4893424583136\n",
      "episode:  920  score:  -1105.8617869526015\n",
      "episode:  940  score:  -1084.0341143542191\n",
      "episode:  960  score:  -1055.3676059038005\n",
      "episode:  980  score:  -1095.5181962501515\n",
      "episode:  1000  score:  -1066.3909370434308\n",
      "episode:  1020  score:  -1058.7115432665132\n",
      "episode:  1040  score:  -1069.8007220647155\n",
      "episode:  1060  score:  -1023.3570082934923\n",
      "episode:  1080  score:  -1041.3400464827669\n",
      "episode:  1100  score:  -1001.0030404454059\n",
      "episode:  1120  score:  -1060.523386010226\n",
      "episode:  1140  score:  -1115.3751204664673\n",
      "episode:  1160  score:  -969.509580732895\n",
      "episode:  1180  score:  -985.1712633217292\n",
      "episode:  1200  score:  -976.4684510989064\n",
      "episode:  1220  score:  -1023.5952250088169\n",
      "episode:  1240  score:  -1004.1030611887829\n",
      "episode:  1260  score:  -1022.6036251750677\n",
      "episode:  1280  score:  -995.6197881351675\n",
      "episode:  1300  score:  -1007.1210998890296\n",
      "episode:  1320  score:  -972.6764122705703\n",
      "episode:  1340  score:  -988.2981436867283\n",
      "episode:  1360  score:  -1101.710735848092\n",
      "episode:  1380  score:  -1057.9701757806845\n",
      "episode:  1400  score:  -985.007368127508\n",
      "episode:  1420  score:  -984.0233739758776\n",
      "episode:  1440  score:  -1029.3539552517664\n",
      "episode:  1460  score:  -924.3680887173226\n",
      "episode:  1480  score:  -958.4221164324875\n",
      "episode:  1500  score:  -952.4921729424602\n",
      "episode:  1520  score:  -1038.5544372243753\n",
      "episode:  1540  score:  -993.0839439462309\n",
      "episode:  1560  score:  -978.6863316019317\n",
      "episode:  1580  score:  -896.3997005657508\n",
      "episode:  1600  score:  -933.8268462842901\n",
      "episode:  1620  score:  -949.0664448913385\n",
      "episode:  1640  score:  -1027.5259739242717\n",
      "episode:  1660  score:  -996.8124555012977\n",
      "episode:  1680  score:  -916.5507649324197\n",
      "episode:  1700  score:  -897.9044154444155\n",
      "episode:  1720  score:  -933.2722633307878\n",
      "episode:  1740  score:  -1061.9374858207327\n",
      "episode:  1760  score:  -907.8005235473535\n",
      "episode:  1780  score:  -952.9663805699689\n",
      "episode:  1800  score:  -910.3460115339755\n",
      "episode:  1820  score:  -954.0100715717723\n",
      "episode:  1840  score:  -834.7421760766545\n",
      "episode:  1860  score:  -914.7452676035011\n",
      "episode:  1880  score:  -905.9625214197094\n",
      "episode:  1900  score:  -1004.7952479930976\n",
      "episode:  1920  score:  -1082.4451820109755\n",
      "episode:  1940  score:  -1013.254592948794\n",
      "episode:  1960  score:  -883.7437545427262\n",
      "episode:  1980  score:  -848.3239285594489\n",
      "episode:  2000  score:  -898.4612246484827\n",
      "episode:  2020  score:  -948.6699559293469\n",
      "episode:  2040  score:  -1081.885783610453\n",
      "episode:  2060  score:  -989.1913549812755\n",
      "episode:  2080  score:  -876.363842658254\n",
      "episode:  2100  score:  -964.9375458824077\n",
      "episode:  2120  score:  -1014.5334930083194\n",
      "episode:  2140  score:  -891.1334270795556\n",
      "episode:  2160  score:  -888.2268302589589\n",
      "episode:  2180  score:  -985.8201992569163\n",
      "episode:  2200  score:  -848.3218013206484\n",
      "episode:  2220  score:  -853.7246686579552\n",
      "episode:  2240  score:  -925.6216234101645\n",
      "episode:  2260  score:  -839.1269355214615\n",
      "episode:  2280  score:  -862.8402816315571\n",
      "episode:  2300  score:  -934.2985500571197\n",
      "episode:  2320  score:  -840.2905232176087\n",
      "episode:  2340  score:  -884.0966350327317\n",
      "episode:  2360  score:  -797.4127889194338\n",
      "episode:  2380  score:  -821.2818303658778\n",
      "episode:  2400  score:  -872.6388526956194\n",
      "episode:  2420  score:  -724.7219946211452\n",
      "episode:  2440  score:  -739.8781673151088\n",
      "episode:  2460  score:  -845.6012279411741\n",
      "episode:  2480  score:  -894.7246120194152\n",
      "episode:  2500  score:  -944.449147792239\n",
      "episode:  2520  score:  -937.5978823891794\n",
      "episode:  2540  score:  -838.2985342266738\n",
      "episode:  2560  score:  -780.5789140526056\n",
      "episode:  2580  score:  -738.5837269430438\n",
      "episode:  2600  score:  -808.3631201842528\n",
      "episode:  2620  score:  -836.1372479466647\n",
      "episode:  2640  score:  -1000.2620362772061\n",
      "episode:  2660  score:  -845.4652343327914\n",
      "episode:  2680  score:  -775.1922365651213\n",
      "episode:  2700  score:  -819.1451349975841\n",
      "episode:  2720  score:  -787.9499045257568\n",
      "episode:  2740  score:  -752.8761304319195\n",
      "episode:  2760  score:  -880.5098196561345\n",
      "episode:  2780  score:  -729.153703033606\n",
      "episode:  2800  score:  -765.1119652037364\n",
      "episode:  2820  score:  -850.1749114455594\n",
      "episode:  2840  score:  -1161.1091978558375\n",
      "episode:  2860  score:  -749.1108720541414\n",
      "episode:  2880  score:  -887.6985740204485\n",
      "episode:  2900  score:  -830.1744669398847\n",
      "episode:  2920  score:  -695.7456036392938\n",
      "episode:  2940  score:  -723.5077815256029\n",
      "episode:  2960  score:  -843.5792243804578\n",
      "episode:  2980  score:  -898.4623462634867\n",
      "episode:  3000  score:  -826.1496120645045\n",
      "episode:  3020  score:  -792.7803902654925\n",
      "episode:  3040  score:  -833.6751993223212\n",
      "episode:  3060  score:  -786.0375120600853\n",
      "episode:  3080  score:  -647.9195610200119\n",
      "episode:  3100  score:  -738.1351728394199\n",
      "episode:  3120  score:  -719.1285059497995\n",
      "episode:  3140  score:  -764.5151800538421\n",
      "episode:  3160  score:  -761.7017082815833\n",
      "episode:  3180  score:  -786.5206572817062\n",
      "episode:  3200  score:  -734.5384520466771\n",
      "episode:  3220  score:  -769.2234636914493\n",
      "episode:  3240  score:  -800.3712490481253\n",
      "episode:  3260  score:  -742.6579468738785\n",
      "episode:  3280  score:  -855.9936484556993\n",
      "episode:  3300  score:  -918.6210870730514\n",
      "episode:  3320  score:  -779.9782327289242\n",
      "episode:  3340  score:  -788.3665030956482\n",
      "episode:  3360  score:  -760.6702100717575\n",
      "episode:  3380  score:  -719.0637499964735\n",
      "episode:  3400  score:  -664.675698974607\n",
      "episode:  3420  score:  -675.4591676631547\n",
      "episode:  3440  score:  -611.2857689257262\n",
      "episode:  3460  score:  -726.1596870183288\n",
      "episode:  3480  score:  -706.4548528371481\n",
      "episode:  3500  score:  -784.6378821196023\n",
      "episode:  3520  score:  -776.8487010993222\n",
      "episode:  3540  score:  -684.9408672117581\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-294e8108ca42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# batch마다 train한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0mreplay_buffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# on-policy method로 업데이트 후 data buffer를 초기화 해준다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-89d4951babea>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(actor, critic, actor_optimizer, critic_optimizer, replay_buffer)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#log_prob = log_pdf(mu, std, action).view(-1) ## G와 shape 맞춰주기 위해서 [data,1] -> [data]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mlog_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_pdf_pytorch_ver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## 위 방식과 같다. 다만 pytorch 라이브러리에 더 의존적.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mtarget_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtd_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ceb32ec3c2c1>\u001b[0m in \u001b[0;36mtd_target\u001b[1;34m(critic, states, rewards, masks)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mtarget_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnext_value\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m## 특정 t 시점부터 next~t에 해당하는 reward가 discounted 됨으로 앞으로 가면서 누적 gamma 곱 해준다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mnext_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtarget_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode = 100000\n",
    "\n",
    "actor_learning_rate = 0.0001\n",
    "critic_learning_rate = 0.001\n",
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "\n",
    "actor = Actor(state_space) # out action은 1로 고정.\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=actor_learning_rate) ## actor에 대한 optimizer Adam으로 설정하기.\n",
    "critic = Critic(state_space)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=critic_learning_rate) ## critic에 대한 optimizer Adam으로 설정하기.\n",
    "\n",
    "batch_size = 32\n",
    "replay_buffer = deque(maxlen=batch_size) # on-policy method로 업데이트 후 data buffer를 초기화 해준다.\n",
    "\n",
    "score = 0\n",
    "step = 0\n",
    "\n",
    "for ep in range(episode):\n",
    "    state = env.reset() # x0\n",
    "    \n",
    "    while True: # episode\n",
    "        \n",
    "        mu, std = actor(torch.Tensor(state))\n",
    "        action = get_action(mu,std)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step([action.item()])\n",
    "        \n",
    "        mask = 0 if done else 1\n",
    "        \n",
    "        replay_buffer.append((state, action, (reward+8)/8, next_state,  mask)) ## 저장\n",
    "        \n",
    "        state = next_state \n",
    "        score += reward\n",
    "        \n",
    "        if step % batch_size == 0 and step != 0:\n",
    "            train(actor, critic, actor_optimizer, critic_optimizer, replay_buffer) # batch마다 train한다.\n",
    "            replay_buffer = deque(maxlen=batch_size) # on-policy method로 업데이트 후 data buffer를 초기화 해준다.\n",
    "        step += 1\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    if ep % 20 == 0 and ep != 0:\n",
    "        print('episode: ',ep,' score: ',score/20.0)\n",
    "        score = 0\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
