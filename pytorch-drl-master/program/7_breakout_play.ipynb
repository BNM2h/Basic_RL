{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7장　벽돌깨기 게임 학습 결과 실행용 프로그램  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구현에 사용할 패키지 임포트\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.spaces.box import Box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구현에 사용할 패키지 임포트\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# 애니메이션을 만드는 함수\n",
    "# 참고 URL http://nbviewer.jupyter.org/github/patrickmineault\n",
    "# /xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1]/72.0*1, frames[0].shape[0]/72.0*1),\n",
    "               dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    " \n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    " \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
    "                                   interval=20)\n",
    " \n",
    "    anim.save('breakout.mp4')  # 애니메이션을 저장하는 부분\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행환경 설정\n",
    "# 参考：https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
    "\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        '''첫 번째 트릭 No-Operation. 초기화 후 일정 단계에 이를때까지 아무 행동도 하지않고\n",
    "        게임 초기 상태를 다양하게 하여 특정 시작 상태만 학습하는 것을 방지한다'''\n",
    "\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(\n",
    "                1, self.noop_max + 1)  # pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        '''두 번째 트릭 Episodic Life. 한번 실패를 게임 종료로 간주하나, 다음 게임을 같은 블록 상태로 시작'''\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        '''5번 실패하면 게임을 완전히 다시 시작'''\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        '''세 번째 트릭 Max and Skip. 4프레임 동안 같은 행동을 지속하되, 3번째와 4번째 프레임의 최댓값 이미지를 관측 obs로 삼는다'''\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros(\n",
    "            (2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        '''네 번째 트릭 Warp frame. DQN 네이처 논문 구현과 같이 84*84 흑백 이미지를 사용'''\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "                                            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height),\n",
    "                           interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "\n",
    "class WrapPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        '''인덱스 순서를 파이토치 미니배치와 같이 조정하는 래퍼'''\n",
    "        super(WrapPyTorch, self).__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            self.observation_space.low[0, 0, 0],\n",
    "            self.observation_space.high[0, 0, 0],\n",
    "            [obs_shape[2], obs_shape[1], obs_shape[0]],\n",
    "            dtype=self.observation_space.dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return observation.transpose(2, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再生用の実行環境\n",
    "\n",
    "\n",
    "class EpisodicLifeEnvPlay(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        '''두 번째 트릭 Episodic Life. 한번 실패를 게임 종료로 간주하나, 다음 게임을 같은 블록 상태로 시작\n",
    "        그러나 여기서는 학습 결과 플레이를 위해 한번 실패에도 블록 상태까지 초기화한다'''\n",
    "\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        # 처음 5개 라이프를 갖고 시작하지만, 하나만 잃어도 종료한다\n",
    "        if self.env.unwrapped.ale.lives() < 5:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        '''한번이라도 실패하면 완전히 초기화'''\n",
    "\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnvPlay(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        '''세 번째 트릭 Max and Skip. 4프레임 동안 같은 행동을 지속하되, 4번째 프레임 이미지를 관측 obs로 삼는다'''\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros(\n",
    "            (2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행환경 생성 함수\n",
    "\n",
    "# 병렬 실행환경\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "\n",
    "\n",
    "def make_env(env_id, seed, rank):\n",
    "    def _thunk():\n",
    "        '''멀티 프로세스로 동작하는 환경 SubprocVecEnv를 실행하기 위해 필요하다'''\n",
    "\n",
    "        env = gym.make(env_id)\n",
    "        #env = NoopResetEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env.seed(seed + rank)  # 난수 시드 설정\n",
    "        #env = EpisodicLifeEnv(env)\n",
    "        env = EpisodicLifeEnvPlay(env)\n",
    "        env = WarpFrame(env)\n",
    "        env = WrapPyTorch(env)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "\n",
    "def make_env_play(env_id, seed, rank):\n",
    "    '''학습 결과 시연용 실행환경'''\n",
    "    env = gym.make(env_id)\n",
    "    #env = NoopResetEnv(env, noop_max=30)\n",
    "    #env = MaxAndSkipEnv(env, skip=4)\n",
    "    env = MaxAndSkipEnvPlay(env, skip=4)\n",
    "    env.seed(seed + rank)  # 난수 시드 설정\n",
    "    env = EpisodicLifeEnvPlay(env)\n",
    "    #env = WarpFrame(env)\n",
    "    #env = WrapPyTorch(env)\n",
    "\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상수 정의\n",
    "\n",
    "ENV_NAME = 'BreakoutNoFrameskip-v4' \n",
    "# Breakout-v0 대신 BreakoutNoFrameskip-v4을 사용\n",
    "# v0은 2~4개 프레임을 자동으로 생략하므로 이 기능이 없는 버전을 사용한다\n",
    "# 참고 URL https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
    "# https://github.com/openai/gym/blob/5cb12296274020db9bb6378ce54276b31e7002da/gym/envs/__init__.py#L371\n",
    "    \n",
    "NUM_SKIP_FRAME = 4 # 생략할 프레임 수\n",
    "NUM_STACK_FRAME = 4  # 하나의 상태로 사용할 프레임의 수\n",
    "NOOP_MAX = 30  #  초기화 후 No-operation을 적용할 최초 프레임 수의 최댓값\n",
    "NUM_PROCESSES = 16 #  병렬로 실행할 프로세스 수\n",
    "NUM_ADVANCED_STEP = 5  # Advanced 학습할 단계 수\n",
    "GAMMA = 0.99  # 시간할인율\n",
    "\n",
    "TOTAL_FRAMES=10e6  #  학습에 사용하는 총 프레임 수\n",
    "NUM_UPDATES = int(TOTAL_FRAMES / NUM_ADVANCED_STEP / NUM_PROCESSES)  # 신경망 수정 총 횟수\n",
    "# NUM_UPDATES는 약 125,000이 됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C 손실함수를 계산하기 위한 상수\n",
    "value_loss_coef = 0.5\n",
    "entropy_coef = 0.01\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "# 최적회 기법 RMSprop에 대한 설정\n",
    "lr = 7e-4\n",
    "eps = 1e-5\n",
    "alpha = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 설정\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 클래스 정의\n",
    "\n",
    "\n",
    "class RolloutStorage(object):\n",
    "    '''Advantage 학습에 사용하는 메모리 클래스'''\n",
    "\n",
    "    def __init__(self, num_steps, num_processes, obs_shape):\n",
    "\n",
    "        self.observations = torch.zeros(\n",
    "            num_steps + 1, num_processes, *obs_shape).to(device)\n",
    "        # *로 리스트의 요소를 풀어낸다(unpack)\n",
    "        # obs_shape→(4,84,84)\n",
    "        # *obs_shape→ 4 84 84\n",
    "\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1).to(device)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1).to(device)\n",
    "        self.actions = torch.zeros(\n",
    "            num_steps, num_processes, 1).long().to(device)\n",
    "\n",
    "        # 할인 총보상을 저장\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1).to(device)\n",
    "        self.index = 0  # 저장할 인덱스\n",
    "\n",
    "    def insert(self, current_obs, action, reward, mask):\n",
    "        '''인덱스가 가리키는 다음 자리에 transition을 저장'''\n",
    "        self.observations[self.index + 1].copy_(current_obs)\n",
    "        self.masks[self.index + 1].copy_(mask)\n",
    "        self.rewards[self.index].copy_(reward)\n",
    "        self.actions[self.index].copy_(action)\n",
    "\n",
    "        self.index = (self.index + 1) % NUM_ADVANCED_STEP  # 인덱스 업데이트\n",
    "\n",
    "    def after_update(self):\n",
    "        '''Advantage 학습 단계 수만큼 단계가 진행되면 가장 최근 단계를 index0에 저장'''\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value):\n",
    "        '''Advantage 학습 단계에 들어가는 각 단계에 대해 할인 총보상을 계산'''\n",
    "\n",
    "        # 주의 : 5번째 단계부터 거슬러 올라가며 계산\n",
    "        # 주의 : 5번째 단계가 Advantage1, 4번째 단계가 Advantage2가 되는 식임\n",
    "        self.returns[-1] = next_value\n",
    "        for ad_step in reversed(range(self.rewards.size(0))):\n",
    "            self.returns[ad_step] = self.returns[ad_step + 1] * \\\n",
    "                GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C 신경망 구성\n",
    "\n",
    "\n",
    "def init(module, gain):\n",
    "    '''결합 가중치를 초기화하는 함수'''\n",
    "    nn.init.orthogonal_(module.weight.data, gain=gain)\n",
    "    nn.init.constant_(module.bias.data, 0)\n",
    "    return module\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    '''합성곱층의 출력 이미지를 1차원으로 변환하는 층'''\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_out):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 결합 가중치 초기화 함수\n",
    "        def init_(module): return init(\n",
    "            module, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        # 합성곱층을 정의\n",
    "        self.conv = nn.Sequential(\n",
    "            # 이미지 크기의 변화 (84*84 -> 20*20)\n",
    "            init_(nn.Conv2d(NUM_STACK_FRAME, 32, kernel_size=8, stride=4)),\n",
    "            # 프레임 4개를 합치므로 input=NUM_STACK_FRAME=4가 된다. 출력은 32이다.\n",
    "            # size 계산  size = (Input_size - Kernel_size + 2*Padding_size)/ Stride_size + 1\n",
    "\n",
    "            nn.ReLU(),\n",
    "            # 이미지 크기의 변화 (20*20 -> 9*9)\n",
    "            init_(nn.Conv2d(32, 64, kernel_size=4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            init_(nn.Conv2d(64, 64, kernel_size=3, stride=1)),  # 이미지 크기의 변화(9*9 -> 7*7)\n",
    "            nn.ReLU(),\n",
    "            Flatten(),  # 이미지를 1차원으로 변환\n",
    "            init_(nn.Linear(64 * 7 * 7, 512)),  # 7*7 이미지 64개를 512차원으로 변환\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 결합 가중치 초기화 함수\n",
    "        def init_(module): return init(module, gain=1.0)\n",
    "\n",
    "        # Critic을 정의\n",
    "        self.critic = init_(nn.Linear(512, 1))  # 출력은 상태가치이므로 1개\n",
    "\n",
    "        # 결합 가중치 초기화 함수\n",
    "        def init_(module): return init(module, gain=0.01)\n",
    "\n",
    "        # Actor를 정의\n",
    "        self.actor = init_(nn.Linear(512, n_out))  # 출력이 행동이므로 출력 수는 행동의 가짓수\n",
    "        \n",
    "        # 신경망을 학습 모드로 전환\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''신경망의 순전파 계산 정의'''\n",
    "        input = x / 255.0  # 이미지의 픽셀값을 [0,255]에서 [0,1] 구간으로 정규화\n",
    "        conv_output = self.conv(input)  # 합성곱층 계산\n",
    "        critic_output = self.critic(conv_output)  # 상태가치 출력 계산\n",
    "        actor_output = self.actor(conv_output)  # 행동 출력 계산\n",
    "\n",
    "        return critic_output, actor_output\n",
    "\n",
    "    def act(self, x):\n",
    "        '''상태 x일때 취할 확률을 확률적으로 구함'''\n",
    "        value, actor_output = self(x)\n",
    "        probs = F.softmax(actor_output, dim=1)    # dim=1で行動の種類方向に計算\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_value(self, x):\n",
    "        '''상태 x의 상태가치를 구함'''\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, x, actions):\n",
    "        '''상태 x의 상태가치, 실제 행동 actions의 로그 확률, 엔트로피를 구함'''\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        log_probs = F.log_softmax(actor_output, dim=1)  # dim=1이므로 행동의 종류 방향으로 계산\n",
    "        action_log_probs = log_probs.gather(1, actions)  # 실제 행동에 대한 log_probs 계산\n",
    "\n",
    "        probs = F.softmax(actor_output, dim=1)  # dim=1이므로 행동의 종류 방향으로 계산\n",
    "        dist_entropy = -(log_probs * probs).sum(-1).mean()\n",
    "\n",
    "        return value, action_log_probs, dist_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트의 두뇌 역할을 하는 클래스로, 모든 에이전트가 공유한다\n",
    "\n",
    "\n",
    "class Brain(object):\n",
    "    def __init__(self, actor_critic):\n",
    "\n",
    "        self.actor_critic = actor_critic  # actor_critic은 Net클래스로 구현한 신경망이다\n",
    "\n",
    "        # 이미 학습된 결합 가중치를 로드하려면\n",
    "        filename = 'weight_end.pth'\n",
    "        #filename = 'weight_112500.pth'\n",
    "        param = torch.load(filename, map_location='cpu')\n",
    "        self.actor_critic.load_state_dict(param)\n",
    "\n",
    "        # 가중치를 학습하는 최적화 알고리즘 설정\n",
    "        self.optimizer = optim.RMSprop(\n",
    "            actor_critic.parameters(), lr=lr, eps=eps, alpha=alpha)\n",
    "\n",
    "    def update(self, rollouts):\n",
    "        '''advanced 학습 대상 5단계를 모두 사용하여 수정한다'''\n",
    "        obs_shape = rollouts.observations.size()[2:]  # torch.Size([4, 84, 84])\n",
    "        num_steps = NUM_ADVANCED_STEP\n",
    "        num_processes = NUM_PROCESSES\n",
    "\n",
    "        values, action_log_probs, dist_entropy = self.actor_critic.evaluate_actions(\n",
    "            rollouts.observations[:-1].view(-1, *obs_shape),\n",
    "            rollouts.actions.view(-1, 1))\n",
    "\n",
    "        # 각 변수의 크기에 주의할 것\n",
    "        # rollouts.observations[:-1].view(-1, *obs_shape) torch.Size([80, 4, 84, 84])\n",
    "        # rollouts.actions.view(-1, 1) torch.Size([80, 1])\n",
    "        # values torch.Size([80, 1])\n",
    "        # action_log_probs torch.Size([80, 1])\n",
    "        # dist_entropy torch.Size([])\n",
    "\n",
    "        values = values.view(num_steps, num_processes,\n",
    "                             1)  # torch.Size([5, 16, 1])\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "\n",
    "        advantages = rollouts.returns[:-1] - values  # torch.Size([5, 16, 1])\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        action_gain = (advantages.detach() * action_log_probs).mean()\n",
    "        # advantages는 detach 하여 정수로 취급한다\n",
    "\n",
    "        total_loss = (value_loss * value_loss_coef -\n",
    "                      action_gain - dist_entropy * entropy_coef)\n",
    "\n",
    "        self.optimizer.zero_grad()  # 경사 초기화\n",
    "        total_loss.backward()  # 역전파 계산\n",
    "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_grad_norm)\n",
    "        # 한번에 결합 가중치가 너무 크게 변화하지 않도록, 경사의 최댓값을 0.5로 제한한다\n",
    "\n",
    "        self.optimizer.step()  # 결합 가중치 수정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakout을 실행하는 환경 클래스\n",
    "\n",
    "NUM_PROCESSES = 1\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def run(self):\n",
    "\n",
    "        # 난수 시드 설정\n",
    "        seed_num = 1\n",
    "        torch.manual_seed(seed_num)\n",
    "        if use_cuda:\n",
    "            torch.cuda.manual_seed(seed_num)\n",
    "\n",
    "        # 실행환경 구축\n",
    "        torch.set_num_threads(seed_num)\n",
    "        envs = [make_env(ENV_NAME, seed_num, i) for i in range(NUM_PROCESSES)]\n",
    "        envs = SubprocVecEnv(envs)  # 멀티프로세스 실행환경\n",
    "\n",
    "        # 모든 에이전트가 공유하는 두뇌 역할 클래스 Brain 객체 생성\n",
    "        n_out = envs.action_space.n  # 행동의 가짓수는 4\n",
    "        actor_critic = Net(n_out).to(device)  # GPU 사용\n",
    "        global_brain = Brain(actor_critic)\n",
    "\n",
    "        # 정보 저장용 변수 생성\n",
    "        obs_shape = envs.observation_space.shape  # (1, 84, 84)\n",
    "        obs_shape = (obs_shape[0] * NUM_STACK_FRAME,\n",
    "                     *obs_shape[1:])  # (4, 84, 84)\n",
    "        # torch.Size([16, 4, 84, 84])\n",
    "        current_obs = torch.zeros(NUM_PROCESSES, *obs_shape).to(device)\n",
    "        rollouts = RolloutStorage(\n",
    "            NUM_ADVANCED_STEP, NUM_PROCESSES, obs_shape)  # rollouts 객체\n",
    "        episode_rewards = torch.zeros([NUM_PROCESSES, 1])  # 현재 에피소드에서 받을 보상 저장\n",
    "        final_rewards = torch.zeros([NUM_PROCESSES, 1])  # 마지막 에피소드의 총 보상 저장\n",
    "\n",
    "        # 초기 상태로 시작\n",
    "        obs = envs.reset()\n",
    "        obs = torch.from_numpy(obs).float()  # torch.Size([16, 1, 84, 84])\n",
    "        current_obs[:, -1:] = obs  # 4번째 프레임에 가장 최근 관측결과를 저장\n",
    "\n",
    "        # advanced 학습에 사용할 객체 rollouts에 첫번째 상태로 현재 상태를 저장\n",
    "        rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "        # 애니메이션 생성용 환경(시연용 추가)\n",
    "        env_play = make_env_play(ENV_NAME, seed_num, 0)\n",
    "        obs_play = env_play.reset()\n",
    "\n",
    "        # 애니메이션 생성을 위해 이미지를 저장할 변수(시연용 추가)\n",
    "        frames = []\n",
    "        main_end = False\n",
    "\n",
    "        # 주 반복문\n",
    "        for j in tqdm(range(NUM_UPDATES)):\n",
    "\n",
    "            # 보상이 기준을 넘어서면 종료 (시연용 추가)\n",
    "            if main_end:\n",
    "                break\n",
    "\n",
    "            # advanced 학습 범위에 들어가는 단계마다 반복\n",
    "            for step in range(NUM_ADVANCED_STEP):\n",
    "\n",
    "                # 행동을 결정\n",
    "                with torch.no_grad():\n",
    "                    action = actor_critic.act(rollouts.observations[step])\n",
    "\n",
    "                cpu_actions = action.squeeze(1).cpu().numpy()  # tensor를 NumPy 변수로\n",
    "\n",
    "                # 1단계를 병렬로 실행, 반환값 obs의 크기는 (16, 1, 84, 84)\n",
    "                obs, reward, done, info = envs.step(cpu_actions)\n",
    "\n",
    "                # 보상을 텐서로 변환한 다음 에피소드 총 보상에 더함\n",
    "                # 크기가 (16,)인 것을 (16, 1)로 변환\n",
    "                reward = np.expand_dims(np.stack(reward), 1)\n",
    "                reward = torch.from_numpy(reward).float()\n",
    "                episode_rewards += reward\n",
    "\n",
    "                # 각 프로세스마다 done이 True이면 0, False이면 1\n",
    "                masks = torch.FloatTensor(\n",
    "                    [[0.0] if done_ else [1.0] for done_ in done])\n",
    "\n",
    "                # 마지막 에피소드의 총 보상을 업데이트\n",
    "                final_rewards *= masks  # done이 True이면 0을 곱하고, False이면 1을 곱하여 리셋\n",
    "                # done이 False이면 0을 더하고, True이면 epicodic_rewards를 더함\n",
    "                final_rewards += (1 - masks) * episode_rewards\n",
    "\n",
    "                # 이미지를 구함(시연용 추가）\n",
    "                obs_play, reward_play, _, _ = env_play.step(cpu_actions[0])\n",
    "                frames.append(obs_play)  # 변환한 이미지를 저장\n",
    "                if done[0]:  # 첫번째 프로세스가 종료된 경우\n",
    "                    print(episode_rewards[0][0].numpy())  # 보상\n",
    "\n",
    "                    # 보상이 300을 초과하면 종료\n",
    "                    if (episode_rewards[0][0].numpy()) > 300:\n",
    "                        main_end = True\n",
    "                        break\n",
    "                    else:\n",
    "                        obs_view = env_play.reset()\n",
    "                        frames = []  # 저장한 이미지를 리셋\n",
    "\n",
    "                # 에피소드의 총 보상을 업데이트\n",
    "                episode_rewards *= masks  # 각 프로세스마다 done이 True이면 0, False이면 1을 곱함\n",
    "\n",
    "                # masks 변수를 GPU로 전달\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                # done이 True이면 모두 0으로\n",
    "                # mask의 크기를 torch.Size([16, 1]) --> torch.Size([16, 1, 1 ,1])로 변환하고 곱함\n",
    "                current_obs *= masks.unsqueeze(2).unsqueeze(2)\n",
    "\n",
    "                # 프레임을 모음\n",
    "                # torch.Size([16, 1, 84, 84])\n",
    "                obs = torch.from_numpy(obs).float()\n",
    "                current_obs[:, :-1] = current_obs[:, 1:]  # 0～2번째 프레임을 1~3번째 프레임으로 덮어씀\n",
    "                current_obs[:, -1:] = obs  # 4번째 프레임에 가장 최근 obs를 저장\n",
    "\n",
    "                # 메모리 객체에 현 단계의 transition을 저장\n",
    "                rollouts.insert(current_obs, action.data, reward, masks)\n",
    "\n",
    "            # advanced 학습의 for문 끝\n",
    "\n",
    "            # advanced 학습 대상 단계 중 마지막 단계의 상태에서 예상되는 상태가치를 계산\n",
    "            with torch.no_grad():\n",
    "                next_value = actor_critic.get_value(\n",
    "                    rollouts.observations[-1]).detach()\n",
    "\n",
    "            # 모든 단계의 할인 총보상을 계산하고, rollouts의 변수 returns를 업데이트\n",
    "            rollouts.compute_returns(next_value)\n",
    "\n",
    "            # 신경망 수정 및 rollout 업데이트\n",
    "            # global_brain.update(rollouts)\n",
    "            rollouts.after_update()\n",
    "\n",
    "        # 주 반복문 끝\n",
    "        display_frames_as_gif(frames)  # 애니메이션 저장 및 재생\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행\n",
    "breakout_env = Environment()\n",
    "frames = breakout_env.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
