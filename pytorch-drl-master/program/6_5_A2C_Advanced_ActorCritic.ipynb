{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 A2C(Advanced Actor-Critic) 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구현에 사용할 패키지 임포트\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상수 정의\n",
    "ENV = 'CartPole-v0'  # 태스크 이름\n",
    "GAMMA = 0.99  # 시간할인율\n",
    "MAX_STEPS = 200  # 1에피소드 당 최대 단계 수\n",
    "NUM_EPISODES = 1000  # 최대 에피소드 수\n",
    "\n",
    "NUM_PROCESSES = 32  # 동시 실행 환경 수\n",
    "NUM_ADVANCED_STEP = 5  # 총 보상을 계산할 때 Advantage 학습을 할 단계 수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C 손실함수 계산에 사용되는 상수\n",
    "value_loss_coef = 0.5\n",
    "entropy_coef = 0.01\n",
    "max_grad_norm = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 클래스 정의\n",
    "\n",
    "\n",
    "class RolloutStorage(object):\n",
    "    '''Advantage 학습에 사용할 메모리 클래스'''\n",
    "\n",
    "    def __init__(self, num_steps, num_processes, obs_shape):\n",
    "\n",
    "        self.observations = torch.zeros(num_steps + 1, num_processes, 4)\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.actions = torch.zeros(num_steps, num_processes, 1).long()\n",
    "\n",
    "        # 할인 총보상 저장\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.index = 0  # insert할 인덱스\n",
    "\n",
    "    def insert(self, current_obs, action, reward, mask):\n",
    "        '''현재 인덱스 위치에 transition을 저장'''\n",
    "        self.observations[self.index + 1].copy_(current_obs)\n",
    "        self.masks[self.index + 1].copy_(mask)\n",
    "        self.rewards[self.index].copy_(reward)\n",
    "        self.actions[self.index].copy_(action)\n",
    "\n",
    "        self.index = (self.index + 1) % NUM_ADVANCED_STEP  # 인덱스 값 업데이트\n",
    "\n",
    "    def after_update(self):\n",
    "        '''Advantage학습 단계만큼 단계가 진행되면 가장 새로운 transition을 index0에 저장'''\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value):\n",
    "        '''Advantage학습 범위 안의 각 단계에 대해 할인 총보상을 계산'''\n",
    "\n",
    "        # 주의 : 5번째 단계부터 거슬러 올라오며 계산\n",
    "        # 주의 : 5번째 단계가 Advantage1, 4번째 단계는 Advantage2가 됨\n",
    "        self.returns[-1] = next_value\n",
    "        for ad_step in reversed(range(self.rewards.size(0))):\n",
    "            self.returns[ad_step] = self.returns[ad_step + 1] * \\\n",
    "                GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C에 사용되는 신경망 구성\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.actor = nn.Linear(n_mid, n_out)  # 행동을 결정하는 부분이므로 출력 갯수는 행동의 가짓수\n",
    "        self.critic = nn.Linear(n_mid, 1)  # 상태가치를 출력하는 부분이므로 출력 갯수는 1개\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''신경망 순전파 계산을 정의'''\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        critic_output = self.critic(h2)  # 상태가치 계산\n",
    "        actor_output = self.actor(h2)  # 행동 계산\n",
    "\n",
    "        return critic_output, actor_output\n",
    "\n",
    "    def act(self, x):\n",
    "        '''상태 x로부터 행동을 확률적으로 결정'''\n",
    "        value, actor_output = self(x)\n",
    "        # dim=1이므로 행동의 종류에 대해 softmax를 적용\n",
    "        action_probs = F.softmax(actor_output, dim=1)\n",
    "        action = action_probs.multinomial(num_samples=1)  # dim=1이므로 행동의 종류에 대해 확률을 계산\n",
    "        return action\n",
    "\n",
    "    def get_value(self, x):\n",
    "        '''상태 x로부터 상태가치를 계산'''\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, x, actions):\n",
    "        '''상태 x로부터 상태가치, 실제 행동 actions의 로그 확률, 엔트로피를 계산'''\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        log_probs = F.log_softmax(actor_output, dim=1)  # dim=1이므로 행동의 종류에 대해 확률을 계산\n",
    "        action_log_probs = log_probs.gather(1, actions)  # 실제 행동의 로그 확률(log_probs)을 구함\n",
    "\n",
    "        probs = F.softmax(actor_output, dim=1)  # dim=1이므로 행동의 종류에 대한 계산\n",
    "        entropy = -(log_probs * probs).sum(-1).mean()\n",
    "\n",
    "        return value, action_log_probs, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트의 두뇌 역할을 하는 클래스. 모든 에이전트가 공유한다\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "class Brain(object):\n",
    "    def __init__(self, actor_critic):\n",
    "        self.actor_critic = actor_critic  # actor_critic은 Net 클래스로 구현한 신경망\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=0.01)\n",
    "\n",
    "    def update(self, rollouts):\n",
    "        '''Advantage학습의 대상이 되는 5단계 모두를 사용하여 수정'''\n",
    "        obs_shape = rollouts.observations.size()[2:]  # torch.Size([4, 84, 84])\n",
    "        num_steps = NUM_ADVANCED_STEP\n",
    "        num_processes = NUM_PROCESSES\n",
    "\n",
    "        values, action_log_probs, entropy = self.actor_critic.evaluate_actions(\n",
    "            rollouts.observations[:-1].view(-1, 4),\n",
    "            rollouts.actions.view(-1, 1))\n",
    "\n",
    "        # 주의 : 각 변수의 크기\n",
    "        # rollouts.observations[:-1].view(-1, 4) torch.Size([80, 4])\n",
    "        # rollouts.actions.view(-1, 1) torch.Size([80, 1])\n",
    "        # values torch.Size([80, 1])\n",
    "        # action_log_probs torch.Size([80, 1])\n",
    "        # entropy torch.Size([])\n",
    "\n",
    "        values = values.view(num_steps, num_processes,\n",
    "                             1)  # torch.Size([5, 16, 1])\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "\n",
    "        # advantage(행동가치-상태가치) 계산\n",
    "        advantages = rollouts.returns[:-1] - values  # torch.Size([5, 16, 1])\n",
    "\n",
    "        # Critic의 loss 계산\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        # Actor의 gain 계산, 나중에 -1을 곱하면 loss가 된다\n",
    "        action_gain = (action_log_probs*advantages.detach()).mean()\n",
    "        # detach 메서드를 호출하여 advantages를 상수로 취급\n",
    "\n",
    "        # 오차함수의 총합\n",
    "        total_loss = (value_loss * value_loss_coef -\n",
    "                      action_gain - entropy * entropy_coef)\n",
    "\n",
    "        # 결합 가중치 수정\n",
    "        self.actor_critic.train()  # 신경망을 학습 모드로 전환\n",
    "        self.optimizer.zero_grad()  # 경사를 초기화\n",
    "        total_loss.backward()  # 역전파 계산\n",
    "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_grad_norm)\n",
    "        # 결합 가중치가 한번에 너무 크게 변화하지 않도록, 경사를 0.5 이하로 제한함(클리핑)\n",
    "\n",
    "        self.optimizer.step()  # 결합 가중치 수정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번에는 에이전트 클래스가 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 환경 클래스\n",
    "import copy\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def run(self):\n",
    "        '''실행 엔트리 포인트'''\n",
    "\n",
    "        # 동시 실행할 환경 수 만큼 env를 생성\n",
    "        envs = [gym.make(ENV) for i in range(NUM_PROCESSES)]\n",
    "\n",
    "        # 모든 에이전트가 공유하는 Brain 객체를 생성\n",
    "        n_in = envs[0].observation_space.shape[0]  # 상태 변수 수는 4\n",
    "        n_out = envs[0].action_space.n  # 행동 가짓수는 2\n",
    "        n_mid = 32\n",
    "        actor_critic = Net(n_in, n_mid, n_out)  # 신경망 객체 생성\n",
    "        global_brain = Brain(actor_critic)\n",
    "\n",
    "        # 각종 정보를 저장하는 변수\n",
    "        obs_shape = n_in\n",
    "        current_obs = torch.zeros(\n",
    "            NUM_PROCESSES, obs_shape)  # torch.Size([16, 4])\n",
    "        rollouts = RolloutStorage(\n",
    "            NUM_ADVANCED_STEP, NUM_PROCESSES, obs_shape)  # rollouts 객체\n",
    "        episode_rewards = torch.zeros([NUM_PROCESSES, 1])  # 현재 에피소드의 보상\n",
    "        final_rewards = torch.zeros([NUM_PROCESSES, 1])  # 마지막 에피소드의 보상\n",
    "        obs_np = np.zeros([NUM_PROCESSES, obs_shape])  # Numpy 배열\n",
    "        reward_np = np.zeros([NUM_PROCESSES, 1])  # Numpy 배열\n",
    "        done_np = np.zeros([NUM_PROCESSES, 1])  # Numpy 배열\n",
    "        each_step = np.zeros(NUM_PROCESSES)  # 각 환경의 단계 수를 기록\n",
    "        episode = 0  # 환경 0의 에피소드 수\n",
    "\n",
    "        # 초기 상태로부터 시작\n",
    "        obs = [envs[i].reset() for i in range(NUM_PROCESSES)]\n",
    "        obs = np.array(obs)\n",
    "        obs = torch.from_numpy(obs).float()  # torch.Size([16, 4])\n",
    "        current_obs = obs  # 가장 최근의 obs를 저장\n",
    "        \n",
    "        # advanced 학습에 사용되는 객체 rollouts 첫번째 상태에 현재 상태를 저장\n",
    "        rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "        # 1 에피소드에 해당하는 반복문\n",
    "        for j in range(NUM_EPISODES*NUM_PROCESSES):  # 전체 for문\n",
    "            # advanced 학습 대상이 되는 각 단계에 대해 계산\n",
    "            for step in range(NUM_ADVANCED_STEP):\n",
    "\n",
    "                # 행동을 선택\n",
    "                with torch.no_grad():\n",
    "                    action = actor_critic.act(rollouts.observations[step])\n",
    "\n",
    "                # (16,1)→(16,) -> tensor를 NumPy변수로\n",
    "                actions = action.squeeze(1).numpy()\n",
    "\n",
    "                # 한 단계를 실행\n",
    "                for i in range(NUM_PROCESSES):\n",
    "                    obs_np[i], reward_np[i], done_np[i], _ = envs[i].step(\n",
    "                        actions[i])\n",
    "\n",
    "                    # episode의 종료가치, state_next를 설정\n",
    "                    if done_np[i]:  # 단계 수가 200을 넘거나, 봉이 일정 각도 이상 기울면 done이 True가 됨\n",
    "\n",
    "                        # 환경 0일 경우에만 출력\n",
    "                        if i == 0:\n",
    "                            print('%d Episode: Finished after %d steps' % (\n",
    "                                episode, each_step[i]+1))\n",
    "                            episode += 1\n",
    "\n",
    "                        # 보상 부여\n",
    "                        if each_step[i] < 195:\n",
    "                            reward_np[i] = -1.0  # 도중에 봉이 넘어지면 페널티로 보상 -1 부여\n",
    "                        else:\n",
    "                            reward_np[i] = 1.0  # 봉이 쓰러지지 않고 끝나면 보상 1 부여\n",
    "\n",
    "                        each_step[i] = 0  # 단계 수 초기화\n",
    "                        obs_np[i] = envs[i].reset()  # 실행 환경 초기화\n",
    "\n",
    "                    else:\n",
    "                        reward_np[i] = 0.0  # 그 외의 경우는 보상 0 부여\n",
    "                        each_step[i] += 1\n",
    "\n",
    "                # 보상을 tensor로 변환하고, 에피소드의 총보상에 더해줌\n",
    "                reward = torch.from_numpy(reward_np).float()\n",
    "                episode_rewards += reward\n",
    "\n",
    "                # 각 실행 환경을 확인하여 done이 true이면 mask를 0으로, false이면 mask를 1로\n",
    "                masks = torch.FloatTensor(\n",
    "                    [[0.0] if done_ else [1.0] for done_ in done_np])\n",
    "\n",
    "                # 마지막 에피소드의 총 보상을 업데이트\n",
    "                final_rewards *= masks  # done이 false이면 1을 곱하고, true이면 0을 곱해 초기화\n",
    "                # done이 false이면 0을 더하고, true이면 episode_rewards를 더해줌\n",
    "                final_rewards += (1 - masks) * episode_rewards\n",
    "\n",
    "                # 에피소드의 총보상을 업데이트\n",
    "                episode_rewards *= masks  # done이 false인 에피소드의 mask는 1이므로 그대로, true이면 0이 됨\n",
    "\n",
    "                # 현재 done이 true이면 모두 0으로 \n",
    "                current_obs *= masks\n",
    "\n",
    "                # current_obs를 업데이트\n",
    "                obs = torch.from_numpy(obs_np).float()  # torch.Size([16, 4])\n",
    "                current_obs = obs  # 최신 상태의 obs를 저장\n",
    "\n",
    "                # 메모리 객체에 현 단계의 transition을 저장\n",
    "                rollouts.insert(current_obs, action.data, reward, masks)\n",
    "\n",
    "            # advanced 학습 for문 끝\n",
    "\n",
    "            # advanced 학습 대상 중 마지막 단계의 상태로 예측하는 상태가치를 계산\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_value = actor_critic.get_value(\n",
    "                    rollouts.observations[-1]).detach()\n",
    "                # rollouts.observations의 크기는 torch.Size([6, 16, 4])\n",
    "\n",
    "            # 모든 단계의 할인총보상을 계산하고, rollouts의 변수 returns를 업데이트\n",
    "            rollouts.compute_returns(next_value)\n",
    "\n",
    "            # 신경망 및 rollout 업데이트\n",
    "            global_brain.update(rollouts)\n",
    "            rollouts.after_update()\n",
    "\n",
    "            # 환경 갯수를 넘어서는 횟수로 200단계를 버텨내면 성공\n",
    "            if final_rewards.sum().numpy() >= NUM_PROCESSES:\n",
    "                print('연속성공')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Episode: Finished after 20 steps\n",
      "1 Episode: Finished after 13 steps\n",
      "2 Episode: Finished after 10 steps\n",
      "3 Episode: Finished after 41 steps\n",
      "4 Episode: Finished after 20 steps\n",
      "5 Episode: Finished after 15 steps\n",
      "6 Episode: Finished after 19 steps\n",
      "7 Episode: Finished after 19 steps\n",
      "8 Episode: Finished after 29 steps\n",
      "9 Episode: Finished after 23 steps\n",
      "10 Episode: Finished after 73 steps\n",
      "11 Episode: Finished after 49 steps\n",
      "12 Episode: Finished after 66 steps\n",
      "13 Episode: Finished after 14 steps\n",
      "14 Episode: Finished after 16 steps\n",
      "15 Episode: Finished after 74 steps\n",
      "16 Episode: Finished after 200 steps\n",
      "17 Episode: Finished after 200 steps\n",
      "18 Episode: Finished after 182 steps\n",
      "19 Episode: Finished after 200 steps\n",
      "연속성공\n"
     ]
    }
   ],
   "source": [
    "# main 실행\n",
    "cartpole_env = Environment()\n",
    "cartpole_env.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
