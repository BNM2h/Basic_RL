{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'a3c_actor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4de6dd764ad0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0ma3c_actor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGlobal_Actor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWorker_Actor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0ma3c_critic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGlobal_Critic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWorker_Critic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'a3c_actor'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "from a3c_actor import Global_Actor, Worker_Actor\n",
    "from a3c_critic import Global_Critic, Worker_Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared global parameters across all workers\n",
    "global_episode_count = 0\n",
    "global_step = 0\n",
    "global_episode_reward = []  # save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3C Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3Cagent(object):\n",
    "\n",
    "    \"\"\"\n",
    "        Global network\n",
    "    \"\"\"\n",
    "    def __init__(self, env_name):\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        K.set_session(self.sess)\n",
    "\n",
    "        # training environment\n",
    "        self.env_name = env_name\n",
    "        self.WORKERS_NUM = multiprocessing.cpu_count() #4\n",
    "\n",
    "        # hyperparameters\n",
    "        self.ACTOR_LEARNING_RATE = 0.0001\n",
    "        self.CRITIC_LEARNING_RATE = 0.001\n",
    "        self.ENTROPY_BETA = 0.01\n",
    "\n",
    "        # get state dimension\n",
    "        env = gym.make(self.env_name)\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        # get action dimension\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        # get action bound\n",
    "        action_bound = env.action_space.high[0]\n",
    "\n",
    "        # create global actor and critic networks\n",
    "        self.global_actor = Global_Actor(self.sess, state_dim, action_dim, action_bound, self.ACTOR_LEARNING_RATE,\n",
    "                                         self.ENTROPY_BETA)\n",
    "        self.global_critic = Global_Critic(self.sess, state_dim, action_dim, self.CRITIC_LEARNING_RATE)\n",
    "\n",
    "        # initialize for later gradient calculation\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def train(self, max_episode_num):\n",
    "\n",
    "        workers = []\n",
    "\n",
    "        # create worker\n",
    "        for i in range(self.WORKERS_NUM):\n",
    "            worker_name = 'worker%i' % i\n",
    "            workers.append(A3Cworker(worker_name, self.env_name, self.global_actor,\n",
    "                                     self.global_critic, max_episode_num))\n",
    "\n",
    "\n",
    "         # create worker (multi-agents) and do parallel training\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.join()\n",
    "\n",
    "        np.savetxt('./save_weights/pendulum_epi_reward.txt', global_episode_reward)\n",
    "        print(global_episode_reward)\n",
    "\n",
    "\n",
    "    ## save them to file if done\n",
    "    def plot_result(self):\n",
    "        plt.plot(global_episode_reward)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3C Worker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3Cworker(threading.Thread):\n",
    "\n",
    "    \"\"\"\n",
    "        local agent network (worker)\n",
    "    \"\"\"\n",
    "    def __init__(self, worker_name, env_name, global_actor, global_critic, max_episode_num):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        #self.lock = threading.Lock()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.GAMMA = 0.95\n",
    "        self.t_MAX = 4 # t-step prediction\n",
    "\n",
    "        self.max_episode_num = max_episode_num\n",
    "\n",
    "        # environment\n",
    "        self.env = gym.make(env_name)\n",
    "        self.worker_name = worker_name\n",
    "\n",
    "        # global network sharing\n",
    "        self.global_actor = global_actor\n",
    "        self.global_critic = global_critic\n",
    "\n",
    "\n",
    "        # get state dimension\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        # get action dimension\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        # get action bound\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "\n",
    "        # create local actor and critic networks\n",
    "        self.worker_actor = Worker_Actor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.worker_critic = Worker_Critic(self.state_dim)\n",
    "\n",
    "        # initial transfer global network parameters to worker network parameters\n",
    "        self.worker_actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "        self.worker_critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "\n",
    "\n",
    "    ## computing Advantages and targets: y_k = r_k + gamma*V(s_k+1), A(s_k, a_k)= y_k - V(s_k)\n",
    "    def n_step_td_target(self, rewards, next_v_value, done):\n",
    "        td_targets = np.zeros_like(rewards)\n",
    "        cumulative = 0\n",
    "        if not done:\n",
    "            cumulative = next_v_value\n",
    "\n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            cumulative = self.GAMMA * cumulative + rewards[k]\n",
    "            td_targets[k] = cumulative\n",
    "        return td_targets\n",
    "\n",
    "\n",
    "    ## convert (list of np.array) to np.array\n",
    "    def unpack_batch(self, batch):\n",
    "        unpack = batch[0]\n",
    "        for idx in range(len(batch) - 1):\n",
    "            unpack = np.append(unpack, batch[idx + 1], axis=0)\n",
    "\n",
    "        return unpack\n",
    "\n",
    "\n",
    "    # train each worker\n",
    "    def run(self):\n",
    "\n",
    "        global global_episode_count, global_step\n",
    "        global global_episode_reward  # total episode across all workers\n",
    "\n",
    "        print(self.worker_name, \"starts ---\")\n",
    "\n",
    "        while global_episode_count <= int(self.max_episode_num):\n",
    "\n",
    "            # initialize batch\n",
    "            batch_state, batch_action, batch_reward = [], [], []\n",
    "\n",
    "            # reset episode\n",
    "            step, episode_reward, done = 0, 0, False\n",
    "            # reset the environment and observe the first state\n",
    "            state = self.env.reset() # shape of state from gym (3,)\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                # visualize the environment\n",
    "                #self.env.render()\n",
    "                # pick an action (shape of gym action = (action_dim,) )\n",
    "                action = self.worker_actor.get_action(state)\n",
    "                # clip continuous action to be within action_bound\n",
    "                action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "                # observe reward, new_state, shape of output of gym (state_dim,)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # change shape (state_dim,) -> (1, state_dim), same to action, next_state\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "                action = np.reshape(action, [1, self.action_dim])\n",
    "\n",
    "                # append to the batch\n",
    "                batch_state.append(state)\n",
    "                batch_action.append(action)\n",
    "                batch_reward.append((reward+8)/8) # <-- normalization\n",
    "                #batch_reward.append(reward)\n",
    "\n",
    "                # update state and step\n",
    "                state = next_state\n",
    "                step += 1\n",
    "                episode_reward += reward[0]\n",
    "\n",
    "                # if batch is full or episode ends, start to train global on batch\n",
    "                if len(batch_state) == self.t_MAX or done:\n",
    "\n",
    "                    # extract states, actions, rewards from batch\n",
    "                    states = self.unpack_batch(batch_state)\n",
    "                    actions = self.unpack_batch(batch_action)\n",
    "                    rewards = self.unpack_batch(batch_reward)\n",
    "\n",
    "                    # clear the batch\n",
    "                    batch_state, batch_action, batch_reward = [], [], []\n",
    "\n",
    "                    # compute n-step TD target and advantage prediction with global network\n",
    "                    next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                    next_v_value = self.global_critic.model.predict(next_state)\n",
    "                    n_step_td_targets = self.n_step_td_target(rewards, next_v_value, done)\n",
    "                    v_values = self.global_critic.model.predict(states)\n",
    "                    advantages = n_step_td_targets - v_values\n",
    "\n",
    "\n",
    "                    #with self.lock:\n",
    "                    # update global critic\n",
    "                    self.global_critic.train(states, n_step_td_targets)\n",
    "                    # update global actor\n",
    "                    self.global_actor.train(states, actions, advantages)\n",
    "\n",
    "                    # transfer global network parameters to worker network parameters\n",
    "                    self.worker_actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "                    self.worker_critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "\n",
    "                    # update global step\n",
    "                    global_step += 1\n",
    "\n",
    "                if done:\n",
    "                    # update global episode count\n",
    "                    global_episode_count += 1\n",
    "                    ## display rewards every episode\n",
    "                    print('Worker name:', self.worker_name, ', Episode: ', global_episode_count,\n",
    "                          ', Step: ', step, ', Reward: ', episode_reward)\n",
    "\n",
    "                    global_episode_reward.append(episode_reward)\n",
    "\n",
    "                    ## save weights every episode\n",
    "                    if global_episode_count % 10 == 0:\n",
    "                        self.global_actor.save_weights(\"./save_weights/pendulum_actor.h5\")\n",
    "                        self.global_critic.save_weights(\"./save_weights/pendulum_critic.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
